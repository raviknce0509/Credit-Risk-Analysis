{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc7011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570570d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape: (110000, 145)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "# Replace 'your_dataset.csv' with your actual file name\n",
    "df = pd.read_csv('/Users/ravikumarnalawade/Documents/Certifications/Project/GitHub/Credit-Risk-Analysis/data/lcDataSample.csv', low_memory=False)\n",
    "\n",
    "print(f\"Initial Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1fbcd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Shape: (109944, 146)\n",
      "Target Distribution:\n",
      " target\n",
      "0    0.860138\n",
      "1    0.139862\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2. Define the Target: Loan Status\n",
    "# We only care about completed loans: 'Fully Paid' vs 'Charged Off'\n",
    "target_mask = df['loan_status'].isin(['Fully Paid', 'Charged Off', 'Default'])\n",
    "df_clean = df[target_mask].copy()\n",
    "\n",
    "# Encode Target: 1 for Bad Loan (Default/Charged Off), 0 for Good\n",
    "df_clean['target'] = df_clean['loan_status'].apply(lambda x: 1 if x in ['Charged Off', 'Default'] else 0)\n",
    "\n",
    "print(f\"Filtered Shape: {df_clean.shape}\")\n",
    "print(\"Target Distribution:\\n\", df_clean['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd00e0d",
   "metadata": {},
   "source": [
    "The Trap: If you build a lazy model that just guesses \"Good Loan\" (0) for everyone, you will get 86% Accuracy.\n",
    "\n",
    "The Reality: That model is useless. It catches 0% of the fraudsters/defaulters, which is the whole point of the business.\n",
    "\n",
    "\n",
    "When I analyzed the target variable, I found a significant class imbalance—only 14% of loans were defaults. This meant Accuracy was a misleading metric. I shifted my optimization focus to Recall (capturing as many defaulters as possible) and ROC-AUC to measure the model's true discriminatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0010a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CRITICAL: Remove Data Leakage\n",
    "# These columns reveal the future. We must drop them to simulate real-world prediction.\n",
    "leakage_cols = [\n",
    "    'recoveries', 'collection_recovery_fee', 'total_pymnt', 'total_pymnt_inv',\n",
    "    'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'last_pymnt_d',\n",
    "    'last_pymnt_amnt', 'next_pymnt_d', 'debt_settlement_flag'\n",
    "]\n",
    "\n",
    "# Drop leakage + original target text column\n",
    "cols_to_drop = leakage_cols + ['loan_status', 'id', 'member_id', 'url', 'desc']\n",
    "# Only drop if they exist in dataset\n",
    "existing_drops = [c for c in cols_to_drop if c in df_clean.columns]\n",
    "df_clean = df_clean.drop(columns=existing_drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f274a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineered: 'credit_hist_years'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/fk3nmszx1jv_9_cy47zq59dc0000gn/T/ipykernel_74535/2725885748.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Engineering: Date Handling\n",
    "# Convert date strings to datetime objects\n",
    "date_cols = ['issue_d', 'earliest_cr_line']\n",
    "for col in date_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "\n",
    "# Feature: Credit History Length (in years)\n",
    "# We calculate the difference between the Loan Issue Date and Earliest Credit Line\n",
    "if 'issue_d' in df_clean.columns and 'earliest_cr_line' in df_clean.columns:\n",
    "    # Remove timezone info from issue_d to match earliest_cr_line\n",
    "    df_clean['issue_d'] = df_clean['issue_d'].dt.tz_localize(None)\n",
    "    df_clean['credit_hist_years'] = (df_clean['issue_d'] - df_clean['earliest_cr_line']).dt.days / 365\n",
    "    print(\"Feature Engineered: 'credit_hist_years'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695e5117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Positively Correlated Features (Higher value = Higher Risk):\n",
      "target                  1.000000\n",
      "dti_joint               0.228373\n",
      "int_rate                0.189151\n",
      "inq_last_12m            0.128271\n",
      "open_rv_24m             0.111657\n",
      "acc_open_past_24mths    0.098334\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Top 5 Negatively Correlated Features (Higher value = Lower Risk):\n",
      "debt_settlement_flag_date   NaN\n",
      "settlement_status           NaN\n",
      "settlement_date             NaN\n",
      "settlement_amount           NaN\n",
      "settlement_percentage       NaN\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 5. Quick EDA Check\n",
    "# Let's look at correlation of numeric features with the target\n",
    "numeric_df = df_clean.select_dtypes(include=[np.number])\n",
    "correlations = numeric_df.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Positively Correlated Features (Higher value = Higher Risk):\")\n",
    "print(correlations.head(6)) # Top 1 is target itself\n",
    "\n",
    "print(\"\\nTop 5 Negatively Correlated Features (Higher value = Lower Risk):\")\n",
    "print(correlations.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf0db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ df_clean exported successfully!\n",
      "  - Pickle: ../data/df_clean.pkl (109944 rows, 131 cols)\n",
      "  - CSV: ../data/df_clean.csv\n",
      "  - Column names: ../data/column_names.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Export df_clean as pickle (preserves dtypes, fastest)\n",
    "df_clean.to_pickle('../data/df_clean.pkl')\n",
    "\n",
    "# Also export as CSV for easy inspection\n",
    "df_clean.to_csv('../data/df_clean.csv', index=False)\n",
    "\n",
    "# Export column names to text file\n",
    "with open('../data/column_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(df_clean.columns))\n",
    "\n",
    "print(f\"✓ df_clean exported successfully!\")\n",
    "print(f\"  - Pickle: ../data/df_clean.pkl ({df_clean.shape[0]} rows, {df_clean.shape[1]} cols)\")\n",
    "print(f\"  - CSV: ../data/df_clean.csv\")\n",
    "print(f\"  - Column names: ../data/column_names.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6.86x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
